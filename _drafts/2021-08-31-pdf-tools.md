---
layout: post
title: PDF processing and analysis with open-source tools
tags: [PDF, VeraPDF, JHOVE, Apache-Tika, preservation-risks]
comment_id: 76
---

<figure class="image">
  <img src="{{ BASE_PATH }}/images/2021/09/plumber-tools.jpg" alt="Plumbers Tool Box">
  <figcaption><a href="https://www.flickr.com/photos/130648318@N06/42662053232">Plumbers Tool Box</a> by <a href="https://www.flickr.com/photos/130648318@N06/">pszz</a> on Flickr. Used under <a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">CC BY-NC-SA 2.0</a>.</figcaption>
</figure>

Problems with PDF files are a recurring theme in my daily work. Over the years, I've been using a variety of open-source software tools for solving such problems. These include tools for validation and error-checking, the extraction of (technical) features and metadata, and text and image extraction, to name but a few. Many of these are tools that I only use occasionally, and I constantly forget how to use them (if I even remember what tool to use at all), even though I've made several lists, cheat-sheets and working notes (but I end up forgetting about those as well!). To fix this situation once and for all, this post brings together my go-to PDF tools and commands for a variety of common tasks in one single place, largely based on my scattered old notes. This is similar to [my 2015 post on preserving optical media from the command-line]({{ BASE_PATH }}/2015/11/13/preserving-optical-media-from-the-command-line).

All of the tools presented here are published as open-source, and most of them have a command-line interface. They all work under Linux (which is the main OS I'm using these days), but most of them are available for other platforms (including Windows) as well.

<!-- more -->

## Disclaimer

I would like to stress that the selection of tasks and tools presented here is by no means meant to be exhaustive, as it is guided to a great degree by the PDF-related issues I've encountered myself in my day to day work. Some of these tasks could be done using other tools (including ones that are not mentioned here), and in some cases these other tools may well be better choices! So there's probably a fair amount of selection bias here, and I don't want to make any claims of presenting the "best" way to do any of these tasks here. Also, many of the example commands in this post can be further refined to particular needs (e.g. using additional options or alternative output formats), and they should probably best seen as (hopefully useful) starting points for the reader's own explorations. My intention is that this post will be very much a "living" document, and that more <strike>PDF madness</strike> content will be added over time.

## PDF multi-tools

Before diving into any specific tasks, let's start with some general-purpose PDF tools and toolkits. Each of these are capable of a wide range of tasks (including some I won't explicitly address here), and they can be seen as "Swiss army-knives" of PDF processing. Whenever I need to get some PDF processing or analysis done and I'm not sure what tool to use, these are usually my starting points. In the majority of cases, at least one of them turns out to have the functionality I'm looking for, so it's a good idea to check them out if you're not familiar with them already.

### Xpdf/Poppler

[Xpdf](https://www.xpdfreader.com/) and [Poppler](https://poppler.freedesktop.org/) are both PDF viewers that include a collection of tools for processing and manipulating PDF files. Poppler is a fork of this software, which adds a number of unique tools that are not part of the original Xpdf package. The tools included with Poppler are:

- **pdfdetach**: lists or extracts embedded files (attachments)
- **pdffonts**: analyzes fonts
- **pdfimages**: extracts images
- **pdfinfo**: displays document information
- **pdfseparate**: page extraction tool
- **pdfsig**: verifies digital signatures
- **pdftocairo**: converts PDF to PNG/JPEG/PDF/PS/EPS/SVG using the [Cairo](https://www.cairographics.org/) graphics library
- **pdftohtml**: converts PDF to HTML
- **pdftoppm**: converts PDF to PPM/PNG/JPEG images
- **pdftops**: converts PDF to PostScript (PS)
- **pdftotext**: text extraction tool
- **pdfunite**: document merging tool

The tools in Xpdf are largely identical, but don't include *pdfseparate*, *pdfsig*, *pdftocairo*, and *pdfunite*. Also, Xpdf has a separate *pdftopng* tool for converting PDF to PNG images (this functionality is covered by *pdftoppn* in the Poppler version). On Debian-based systems the Poppler tools are part of the package *poppler-utils*.

### Pdfcpu

[Pdfcpu](https://pdfcpu.io/) is a PDF processor that is written in the *Go* language. The documentation explicity mentions its main focus is strong support for batch processing and scripting via a rich command line. It supports all PDF versions up to PDF 1.7 (ISO-32000).

### Apache PDFBox

[Apache PDFBox](https://pdfbox.apache.org/) is an open source Java library for working with PDF documents. It includes a set of [command-line tools](https://pdfbox.apache.org/2.0/commandline.html) for various PDF processing tasks. Binary distributions (as [JAR](https://en.wikipedia.org/wiki/JAR_(file_format)) packages) are available [here](https://pdfbox.apache.org/download.html) (you'll need the "standalone" JARs).

### QPDF

[QPDF](http://qpdf.sourceforge.net/) is "a command-line program that does structural, content-preserving transformations on PDF files".

### MuPDF

[MuPDF](https://www.mupdf.com/) is "a lightweight PDF, XPS, and E-book viewer". It includes the [mutool](https://www.mupdf.com/docs/index.html) utility, which can do a number of PDF processing tasks.

### Ghostscript

[Ghostscript](https://www.ghostscript.com/) is "an interpreter for the PostScript language and PDF files". It provides rendering to a variety of raster and vector formats.  

The remaining sections of this post are dedicated to specific tasks. As you will see, many of these can be addressed using the multi-tools listed in this section.

## Validation integrity testing

PDFs that are damaged, structurally flawed or otherwise not conformant to the PDF format specification can result in a multitude of problems. A number of tools provide error checking and integrity testing functionality. This can range from limited structure checks, to full (claimed) validation against the filespec. It's important to note that none of the tools mentioned here are perfect, and some faults that are picked up by one tool may be completely ignored by another one and vice versa. So it's often a good idea to try multiple tools.

### Validate with Pdfcpu

The Pdfcpu command-line tool has a [`validate` command](https://pdfcpu.io/core/validate) that checks a file's compliance against [PDF 32000-1:2008](https://www.adobe.com/content/dam/acom/en/devnet/pdf/pdfs/PDF32000_2008.pdf) (i.e. the ISO version of PDF 1.7). It provides both a "strict" and a "relexed" validation mode, where the "relaxed" mode (which is the default!) ignores some common violations of the PDF specification. The command-line is:

```bash
pdfcpu validate whatever.pdf
```

The "strict" mode can be activated with the `-m` option:

```bash
pdfcpu validate -m strict whatever.pdf
```

### Validate with JHOVE

[JHOVE](http://jhove.openpreservation.org/) is a is a file format identification, validation and characterisation tool that includes a module for PDF validation. It is widely used in the digital heritage (libraries, archives) sector. Here's a typical command-line example (note that you explicitly need to invoke the *PDF-hul* module via the `-m` option; omitting this can give unexpected results):

```bash
jhove -m PDF-hul -i whatever.pdf
```

Check out the [documentation](https://jhove.openpreservation.org/modules/pdf/) for more information about JHOVE's PDF module, and its limitations.

### Check integrity with QPDF 

The `--check` option of QPDF (see above) performs checks on a PDF's overall file structure. QPDF does not provide full-fledged validation, and the [documentation](http://qpdf.sourceforge.net/files/qpdf-manual.html) states that:

> A file for which --check reports no errors may still have errors in stream data content but should otherwise be structurally sound

Nevertheless, QPDF is still useful for detecting various issues, especially in conjunction with the `--verbose` option. Here's an example command-line:

```bash
qpdf --check --verbose whatever.pdf
```

### Check for Ghostscript rendering errors

Another useful technique is to process a PDF with Ghostscript (rendering the result to a "nullpage" device). For example: 

```bash
gs -dNOPAUSE -dBATCH -sDEVICE=nullpage whatever.pdf
```

In case of any problems with the input file, Ghostscript will report quite detailed information. As an example, here's the output for a PDF with a truncated document trailer:

```
   **** Error:  An error occurred while reading an XREF table.
   **** The file has been damaged.  This may have been caused
   **** by a problem while converting or transfering the file.
   **** Ghostscript will attempt to recover the data.
   **** However, the output may be incorrect.
   **** Warning:  There are objects with matching object and generation
   **** numbers.  The output may be incorrect.
   **** Error:  Trailer dictionary not found.
                Output may be incorrect.
   No pages will be processed (FirstPage > LastPage).

   **** This file had errors that were repaired or ignored.
   **** Please notify the author of the software that produced this
   **** file that it does not conform to Adobe's published PDF
   **** specification.
```

### Check for errors with Mutool info command

Running Mutool (part of MuPDF, see above) with the `info` command returns information about internal pdf resources. In case of broken or malformed files the output includes error messages, which can be quite informative. Here's an example command-line:

```bash
mutool info whatever.pdf 
```

### Other options

- [VeraPDF](https://verapdf.org/) can provide useful information on damaged or invalid PDF documents. However, VeraPDF is primarily aimed at validation against [PDF/A](https://en.wikipedia.org/wiki/PDF/A) and [PDF/UA](https://en.wikipedia.org/wiki/PDF/UA) profiles, which are both subsets of [ISO 32000](https://en.wikipedia.org/wiki/PDF) (which defines the PDF format's full feature set). As a result, VeraPDF's validation output can be somewhat difficult to interpret for "regular" PDFS (i.e. documents that are not PDF/A or PDF/UA). Nevertheless, experienced users may find VeraPDF useful for such files as well.

- Several online resources recommend the *pdfinfo* tool that is part of Xpdf and Poppler for integrity checking. However, while writing this post I ran a quick test of the tool on a PDF with a truncated document trailer[^1] (which is a very serious flaw), which was not flagged by *pdfinfo* at all.  

## Validate compliance with PDF/A or PDF/UA with VeraPDF

[PDF/A](https://en.wikipedia.org/wiki/PDF/A) comprises a set of ISO-standardized profiles that are aimed at long-term preservation. [PDF/UA](https://en.wikipedia.org/wiki/PDF/UA) is another ISO-standardized profile that ensures accessibility for people with disabilities. These are not separate file formats, but rather profiles within ISO 32000 that put some constraints on PDF's full set of features. [VeraPDF](https://verapdf.org/) was originally developed as an open source PDF/A validator that  covers all parts of the PDF/A standards. Starting with version 1.18, it also added support for PDF/UA. The following command lists al available validation profiles:

```bash
verapdf -l
```

Result:

```
  1a - PDF/A-1A validation profile
  1b - PDF/A-1B validation profile
  2a - PDF/A-2A validation profile
  2b - PDF/A-2B validation profile
  2u - PDF/A-2U validation profile
  3a - PDF/A-3A validation profile
  3b - PDF/A-3B validation profile
  3u - PDF/A-3U validation profile
  ua1 - PDF/UA-1 validation profile
```

When running VeraPDF, use the `-f` (flavour) option to set the desired validation profile. For example, for PDF/A-1A use something like this[^2]:

```bash
verapdf -f 1a whatever.pdf > whatever-1a.xml
```

And for PDF/UA:

```bash
verapdf -f ua1 whatever.pdf > whatever-ua.xml
```

The [documentation](https://docs.verapdf.org/cli/validation/) provides more detailed instructions on how to use VeraPDF.

## Document information and metadata extraction

A large number of tools are capable of displaying or extracting technical characteristics and various kinds of metadata, with varying degrees of detail. I'll only highlight a few here.

### Extract general characteristics with pdfinfo

The *pdfinfo* tool that is part of Xpdf and Poppler is useful for a quick overview of a document's general characteristics. The basic command line is:

```bash
pdfinfo whatever.pdf
```

Which gives the following result:

```
Creator:        PdfCompressor 3.1.32
Producer:       CVISION Technologies
CreationDate:   Thu Sep  2 07:52:56 2021 CEST
ModDate:        Thu Sep  2 07:53:20 2021 CEST
Tagged:         no
UserProperties: no
Suspects:       no
Form:           none
JavaScript:     no
Pages:          1
Encrypted:      no
Page size:      439.2 x 637.92 pts
Page rot:       0
File size:      24728 bytes
Optimized:      yes
PDF version:    1.6
```

### Elaborate feature extraction with VeraPDF

Although primarily aimed at PDF/A validation, [VeraPDF](https://verapdf.org/) can also be used as a powerful metadata and feature extractor for any PDF file (including files that don't follow the PDF/A or PDF/UA at all!). By default, VeraPDF is configured to only extract metadata from a PDF's information dictionary, but this behaviour can be easily changed by modifying a configuration file, which is [explained in the documentation](https://docs.verapdf.org/cli/config/#features.xml). This enables you to obtain detailed information about things like Actions, Annotations, colour spaces, document security features (including encryption), embedded files, fonts, images, and much more. Then use a command line like[^3]:

```
verapdf --off --extract whatever.pdf > whatever.xml
```

VeraPDF can also be used to recursively process all files with a .pdf extension in a directory tree, using the following command-line (here, *myDir* is the root of the directory tree):

```
verapdf --recurse --off --extract myDir > whatever.xml
```

The [VeraPDF documentation](https://docs.verapdf.org/cli/feature-extraction/) discusses the feature extraction functionality in more detail.

## Policy or profile compliance assessment with VeraPDF

The results of the feature extraction exercise described in the previous section can also be used as input for policy-based assessments. For instance, archival institutions may have policies that prohibit e.g. PDFs with encryption or fonts that are not embedded. This can also be done with VeraPDF. This requires that the rules that make up the policy are expressed as a machine-readable [Schematron](https://en.wikipedia.org/wiki/Schematron) file. As an example, the Schematron file below is made up of two rules that each prohibit specific encryption-related features:

```xml
<?xml version="1.0"?>

<sch:schema xmlns:sch="http://purl.oclc.org/dsdl/schematron" queryBinding="xslt">
    <sch:pattern name="Disallow encrypt in trailer dictionary">
        <sch:rule context="/report/jobs/job/featuresReport/documentSecurity">
            <sch:assert test="not(encryptMetadata = 'true')">Encrypt in trailer dictionary</sch:assert>
        </sch:rule>
    </sch:pattern>    
    
    <sch:pattern name="Disallow other forms of encryption (e.g. open password)">
        <sch:rule context="/report/jobs/job/taskResult/exceptionMessage">
            <sch:assert test="not(contains(.,'encrypted'))">Encrypted document</sch:assert>
        </sch:rule>
    </sch:pattern>
    
</sch:schema>
```

A PDF can subsequently be tested against these rules (here in the file "policy.sch") using the following basic command-line:

```bash
verapdf --extract --policyfile policy.sch whatever.pdf > whatever.xml
```

The outcome of the policy-based assessment can be found in the output file's *policyReport* element. In the example below, the PDF did not meet one of the rules:

```xml
<policyReport passedChecks="0" failedChecks="1" xmlns:vera="http://www.verapdf.org/MachineReadableReport">
    <passedChecks/>
    <failedChecks>
        <check status="failed" test="not(encryptMetadata = 'true')" location="/report/jobs/job/featuresReport/documentSecurity">
            <message>Encrypt in trailer dictionary</message>
        </check>
    </failedChecks>
</policyReport>
```

More examples can be found in my 2017 post [Policy-based assessment with VeraPDF - a first impression]({{ BASE_PATH }}/2017/06/01/policy-based-assessment-with-verapdf-a-first-impression).

## Text extraction

Text extraction from PDF documents is notoriously hard. [This post]](https://filingdb.com/b/pdf-text-extraction) gives a good overview of the main pitfalls. With that said, quite a few tools are available for this, and below I list a few that are useful starting points.

### Extract text with pdftotext

The *pdftotext* tool that is part of Poppler and Xpdf is a good starting point. The basic command-line is:

```bash
pdftotext whatever.pdf whatever.txt
```

The tool has lots of options to fine-tune the default behaviour, so make sure to check those out if you're looking for. Note that the available options vary somewhat between the Poppler and Xpdf versions. The [documentation of the Poppler version is available here](https://manpages.debian.org/stretch/poppler-utils/pdftotext.1.en.html), and [here is the Xpdf version](https://www.xpdfreader.com/pdftotext-man.html).

### Extract text with PDFBox

PDFBox is also a good choice for text extraction. Here's an example command (you may need to adapt the path to the JAR file and its name according to the location and version on your system):

```bash
java -jar ~/pdfbox/pdfbox-app-2.0.24.jar ExtractText whatever.pdf whatever.txt
```

PDFBox also provides various options, which are [documented here](https://pdfbox.apache.org/1.8/commandline.html#extracttext).

### Extract text with Apache Tika

[Apache Tika](https://tika.apache.org/) is a Java library that supports metadata and content (including text) extraction for a wide variety of file formats. For PDF it uses the PDF parser of PDFBox (see previous section). Tika is particularly interesting for situations where text extraction from multiple input formats is needed. For command-line use, download the *Tika-app* runnable JAR from [here](https://tika.apache.org/download.html). By default, Tika will extract both text and metadata, and report both in XHTML format. You can change this behaviour with the `--text` option (as with PDFBox, you may need to adapt the path and name of the JAR file):

```bash
java -jar ~/tika/tika-app-2.1.0.jar --text whatever.pdf > whatever.txt
```

An explanation of all available options is [available here](https://tika.apache.org/1.17/gettingstarted.html) (section "Using Tika as a command line utility").

If you need to process many PDFs, you might consider using TikaServer for better performance. A ([runnable JAR is available here](https://tika.apache.org/download.html)). To use it, first start the server using:

```bash
java -jar ~/tika/tika-server-standard-2.1.0.jar
```
Once the server is running, use [cURL](https://en.wikipedia.org/wiki/CURL) (from another terminal window) to submit text extraction requests:

```bash
curl -T whatever.pdf http://localhost:9998/tika --header "Accept: text/plain" > whatever.txt

```

The full TikaServer documentation is [available here](https://cwiki.apache.org/confluence/display/TIKA/TikaServer).

Yet another option is [Tika-python](https://github.com/chrismattmann/tika-python), which is a Python port of Tika that uses TikaServer under the hood (resulting in similar performance).

## Link extraction

When extracting (hyper)links, it's important to make a distinction between the following two cases:

1. Links that are encoded as a "link annotation", which is a data structure in PDF that results in a clickable link
2. Non-clickable links/URLs that are just part of the body text.

The automated extraction of the first case is straightforward, while the second case depends on some kind of lexical analysis of the body text (typically based on regular expressions). For most practical applications the extraction of both types is desired.

### Extract links with pdfx

The [pdfx](https://www.metachris.com/pdfx/) tool is designed to detect and extract external references, including URLs. Its URL detection uses lexical analysis, and is based on [RegEx patterns written by John Gruber](https://gist.github.com/gruber/8891611). The basic command line for URL extraction is:

```bash
pdfx -v whatever.pdf > whatever.txt
```

I did some limited testing with this tool in 2016. One issue I ran into is that pdfx [truncates URLS that span more than one line](https://github.com/metachris/pdfx/issues/21). As of 2021, this issue hasn't been fixed so far, which seriously limits the usefulness of this (otherwise very interesting) tool. It's worth mentioning that *pdfx* also provides functionality to automatically download all referenced PDFs from any PDF document. I haven't tested this myself.

### Other link extraction tools

- Some years ago Ross Spencer wrote [a link extraction tool that uses Apache Tika](https://github.com/httpreserve/tikalinkextract). There's more info in [this blog post](https://openpreservation.org/blogs/hyperlinks-in-your-files-how-to-get-them-out-using-tikalinkextract/). I'm not sure if this tool is still maintained.

- Around the same time I wrote [this simple extraction script](https://gist.github.com/bitsgalore/aab680a9bccfc5496948b776ee06397c) that wraps around Apache Tika and the [xurl](https://github.com/mvdan/xurls) tool. I used this to extract URLs from MS Word documents, but this should probably work for PDF too (I haven't tested this though!).

## Check PDF document for broken links with pdfcpu

<https://gist.github.com/bitsgalore/9eac23e44d6d99b3caa6#check-pdf-document-for-broken-links-with-pdfcpu>

## Image extration with pdfimages

PDFs often contain embedded images, which can be extracted with *pdfimages* tool that is part of Xpdf/Poppler. At minimum, it takes as its arguments the name of the input PDF document, and the "image-root" which is actually just a text prefix that is used to generate the name of the output images. By default it writes its output to one of the [Netpbm](https://en.wikipedia.org/wiki/Netpbm) file formats, but for convenience you might want to use the `-png` option, which uses the PNG format instead:

```bash
pdfimages -png whatever.pdf whatever
```

Output images are now written as "whatever-000.png", "whatever-001.png", "whatever-002.png", and so on. The `-j`, `-jp2`, `-jbig2` and `-ccitt` switches can be used to store JPEG, JPEG2000, JBIG2 and CCITT images in their native formats, respectively (or use `-all`, which combines all of these options). 

<!--
### PDFBox

```bash
java -jar ~/pdfbox/pdfbox-app-2.0.24.jar ExtractImages whatever.pdf
```

This gave me the following error for a PDF with an embedded JPEG 2000 image:

```
SEVERE: Cannot read JPEG2000 image: Java Advanced Imaging (JAI) Image I/O Tools are not installed
```

-->

## Listy embedded image information with pdfimages

The *pdfimages* tool is also useful for getting an overview of all embedded images in a PDF, and their main characteristics (width, height, colour, encoding, resolution and size). just user the `-list` option as shown below:  

```bash
pdfimages -list whatever.pdf
```

This results in a nice table like this:

```
page   num  type   width height color comp bpc  enc interp
-----------------------------------------------------------
   1     0 image    1830  2658  gray    1   1  jbig2  no
   1     1 image     600   773  gray    1   8  jpx    no



page   object ID x-ppi y-ppi size ratio
----------------------------------------
   1       16  0   301   301   99B 0.0%
   1       17  0   300   300 17.9K 4.0%
```

## PDF comparison with Comparepdf

The [Comparepdf](http://www.qtrac.eu/)[^4] tool compares pairs of PDFs, based on either text or visual appearance. By default it uses the program exit code to store the result of the comparison. The tool's command-line help text explains the possible outcomes:

> A return value of 0 means no differences detected; 1 or 2 signifies an error; 10 means they differ visually, 13 means they differ textually, and 15 means they have different page counts

For clarity I used the `-v` switch in the examples below, which activates verbose output. To test if two PDFs contain the same text, use:

```bash
comparepdf -ct -v=2 whatever.pdf wherever.pdf
```

If al goes well the output is either "No differences detected" or "Files have different texts".

To compare the visual appearance of two PDFs, use:

```bash
comparepdf -ca -v=2 whatever.pdf wherever.pdf
```

In this case the output either shows "No differences detected" or "Files look different".

## Low-level PDF hacking

From time to time some pesky PDF issue arises the need to look into the low-level objects that make up a document. I mostly use a Hex editor for such cases, but here are some useful additional tools.

### Inspect internal PDF structure with PDFBox PDFDebugger

PDFBox includes a "PDF Debugger" that is quite useful for exploring a PDF's low-level structure. You can start it with the following command:

```bash
java -jar ~/pdfbox/pdfbox-app-2.0.24.jar PDFDebugger whatever.pdf
```

Subsequently a GUI window pops up that allows you to browse the PDF's internal objects:

<figure class="image">
  <img src="{{ BASE_PATH }}/images/2021/09/pdf-debugger.png" alt="PDF Debugger screenshot">
  <figcaption>Screenshot of PDFBOX PDFDebugger.</figcaption>
</figure>

## View, search and extract PDF objects with mutool show

Mutool's *show* command allows you to print user-defined PDF objects to stdout. A couple of things you can do with this:

- Print the document trailer:
  ```bash
  mutool show whatever.pdf trailer
  ```

  Result:

  ```
trailer
<<
  /DecodeParms <<
    /Columns 3
    /Predictor 12
  >>
  /Filter /FlateDecode
  /ID [ <500AB94E8F45C149808B2EEE98528B78> <431017E495216040A953126BB73D0CD4> ]
  /Index [ 11 10 ]
  /Info 10 0 R
  /Length 47
  /Prev 24426
  /Root 12 0 R
  /Size 21
  /Type /XRef
  /W [ 1 2 0 ]
>>
  ```

- Print the cross-reference table:
  ```bash
  mutool show whatever.pdf xref
  ```

  Result:
  ```
  xref
  0 21
  00000: 0000000000 00000 f 
  00001: 0000019994 00000 n 
  00002: 0000020399 00000 n 
  00003: 0000020534 00000 n 
  ::
  etc
  ```

- Print an indirect object by its number:
  ```bash
  mutool show whatever.pdf 12
  ```

  Result:

  ```
12 0 obj
<<
  /Metadata 4 0 R
  /Pages 9 0 R
  /Type /Catalog
>>
endobj
```

- Extract only stream contents as raw binary data and write to a new file:
  ```bash
  mutool show -b whatever.pdf 151 > whatever.dat
  ```

  This command is particularly useful for extracting the raw data from a stream object (e.g. an image or multimedia file).

More advanced queries are possible as well. For example, the [mutool manual](https://mupdf.com/docs/manual-mutool-show.html) gives the following example, which shows all JPEG compressed stream objects in a file:

```bash
mutool show whatever.pdf grep | grep '/Filter/DCTDecode'
```

Result:

```
1 0 obj <</BitsPerComponent 8/ColorSpace/DeviceRGB/Filter/DCTDecode/Height 516/Length 76403/Subtype/Image/Type/XObject/Width 1226>> stream
18 0 obj <</BitsPerComponent 8/ColorSpace/DeviceRGB/Filter/DCTDecode/Height 676/Length 149186/Subtype/Image/Type/XObject/Width 1014>> stream
19 0 obj <</BitsPerComponent 8/ColorSpace/DeviceRGB/Filter/DCTDecode/Height 676/Length 142232/Subtype/Image/Type/XObject/Width 1014>> stream
24 0 obj <</BitsPerComponent 8/ColorSpace/DeviceRGB/Filter/DCTDecode/Height 676/Length 192073/Subtype/Image/Type/XObject/Width 1014>> stream
25 0 obj <</BitsPerComponent 8/ColorSpace/DeviceRGB/Filter/DCTDecode/Height 676/Length 141081/Subtype/Image/Type/XObject/Width 1014>> stream
```

## Repair

Ghostscript, Poppler:

<https://superuser.com/questions/278562/how-can-i-fix-repair-a-corrupted-pdf-file>


## Convert PDF to sth else

pdftocairo (Poppler)?


## Repair corrupted PDF

<https://superuser.com/questions/278562/how-can-i-fix-repair-a-corrupted-pdf-file>


## Reduce size of large PDF with hi-res graphics

## Further resources

- [Moritz Mähr, "Working with batches of PDF files," The Programming Historian 9 (2020)](https://doi.org/10.46430/phen0088)
- [PDF tools in Community Owned Digital Preservation Tool Registry (COPTR)](https://coptr.digipres.org/index.php/PDF)
- [Policy-based assessment with VeraPDF - a first impression]({{ BASE_PATH }}/2017/06/01/policy-based-assessment-with-verapdf-a-first-impression)
- [What's so hard about PDF text extraction? ​](https://filingdb.com/b/pdf-text-extraction)

[^1]: Command line: `pdfinfo whatever.pdf`

[^2]: In this example output is redirected to a file; this is generally a good idea because of the amount of XML output generated by VeraPDF.

[^3]: The `--off` switch disables PDF/A validation. Output is redirected to a file (recommended because, depending on the configuration used, VeraPDF can generate a *lot* of output).

[^4]: On Debian-based systems you can install it using `sudo apt install comparepdf`.

[^5]: Whith the "regular" Tika application, performance can be poor because a new Java VM has to be started for each processed PDF. With TikaServer, the VM is only started once. 